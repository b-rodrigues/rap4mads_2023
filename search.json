[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "",
    "text": "Introduction\nThis is the 2023 edition of the course. If you’re looking for the 2022 edition, you can click here\nThis course is based on my book titled Building Reproducible Analytical Pipelines with R. This course focuses only on certain aspects that are discussed in greater detail in the book."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Schedule",
    "text": "Schedule\n\n2022/10/25, morning: Introduction and data exploration with R\n2022/10/25, afternoon: Functional programming\n2022/10/31, afternoon: Git (optional?)\n2022/11/07, afternoon: Package development and unit testing\n2022/11/08, morning: Build automation\n2022/11/08, afternoon: Building Data products\n2022/11/21, afternoon: Self-contained pipelines with Docker\n2022/11/22, morning: CI/CD with Github Actions"
  },
  {
    "objectID": "index.html#reproducible-analytical-pipelines",
    "href": "index.html#reproducible-analytical-pipelines",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Reproducible analytical pipelines?",
    "text": "Reproducible analytical pipelines?\nThis course is my take on setting up code that results in some data product. This code has to be reproducible, documented and production ready. Not my original idea, but introduced by the UK’s Analysis Function.\nThe basic idea of a reproducible analytical pipeline (RAP) is to have code that always produces the same result when run, whatever this result might be. This is obviously crucial in research and science, but this is also the case in businesses that deal with data science/data-driven decision making etc.\nA well documented RAP avoids a lot of headache and is usually re-usable for other projects as well."
  },
  {
    "objectID": "index.html#data-products",
    "href": "index.html#data-products",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Data products?",
    "text": "Data products?\nIn this course each of you will develop a data product. A data product is anything that requires data as an input. This can be a very simple report in PDF or Word format or a complex web app. This website is actually also a data product, which I made using the R programming language. In this course we will not focus too much on how to create automated reports or web apps (but I’ll give an introduction to these, don’t worry) but our focus will be on how to set up a pipeline that results in these data products in a reproducible way."
  },
  {
    "objectID": "index.html#machine-learning",
    "href": "index.html#machine-learning",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Machine learning?",
    "text": "Machine learning?\nNo, being a master in machine learning is not enough to become a data scientist. Actually, the older I get, the more I think that machine learning is almost optional. What is not optional is knowing how:\n\nto write, test, and properly document code;\nto acquire (reading in data can be tricky!) and clean data;\nto work inside the Linux terminal/command line interface;\nto use Git, Docker for Dev(Git)Ops;\nthe Internet works (what’s a firewall? what’s a reverse proxy? what’s a domain name? etc, etc…);\n\nBut what about machine learning? Well, depending what you’ll end up doing, you might indeed focus a lot on machine learning and/or statistical modeling. That being said, in practice, it is very often much more efficient to let some automl algorithm figure out the best hyperparameters of a XGBoost model and simply use that, at least as a starting point (but good luck improving upon automl…). What matters, is that the data you’re feeding to your model is clean, that your analysis is sensible, and most importantly, that it could be understood by someone taking over (imagine you get sick) and rerun with minimal effort in the future. The model here should simply be a piece that could be replaced by another model without much impact. The model is rarely central… but of course there are exceptions to this, especially in research, but every other point I’ve made still stands. It’s just that not only do you have to care about your model a lot, you also have to care about everything else.\nSo in this course we’re going to learn a bit of all of this. We’re going to learn how to write reusable code, learn some basics of the Linux command line, Git and Docker."
  },
  {
    "objectID": "index.html#why-r-why-not-insert-your-favourite-programming-language",
    "href": "index.html#why-r-why-not-insert-your-favourite-programming-language",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Why R? Why not [insert your favourite programming language]",
    "text": "Why R? Why not [insert your favourite programming language]\nIn my absolutely objective opinion R is currently the most interesting and simple language you can use to create such data products. If you learn R you have access to almost 19’000 packages (as of October 2022) to:\n\nclean data (see: {dplyr}, {tidyr}, {data.table}…);\nwork with medium and big data (see: {arrow}, {sparklyr}…);\nvisualize data (see: {ggplot2}, {plotly}, {echarts4r}…);\ndo literate programming (using Rmarkdown or Quarto, you can write books, documents even create a website);\ndo functional programming (see: {purrr}…);\ncall other languages from R (see: {reticulate} to call Python from R);\ndo machine learning and AI (see: {tidymodels}, {tensorflow}, {keras}…)\ncreate webapps (see: {shiny}…)\ndomain specific statistics/machine learning (see CRAN Task Views for an exhaustive list);\nand more\n\nIt’s not just about what the packages provide: installing R and its packages and dependencies is rarely frustrating, which is not the case with Python (Python 2 vs Python 3, pip vs conda, pyenv vs venv…, dependency hell is a real place full of snakes)\n\n\n\n\n\n\n\nThat doesn’t mean that R does not have any issues. Quite the contrary, R sometimes behaves in seemingly truly bizarre ways (as an example, try running nchar(\"1000000000\") and then nchar(1000000000) and try to make sense of it). To know more about such bizarre behaviour, I recommend you read The R Inferno (linked at the end of this chapter). So, yes, R is far from perfect, but it sucks less than the alternatives (again, in my absolutely objective opinion)."
  },
  {
    "objectID": "index.html#pre-requisites",
    "href": "index.html#pre-requisites",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nI will assume basic programming knowledge, and not much more. If you need to set up R on your computer you can read the intro to my other book Modern R with the tidyverse. Follow the pre-requisites there: install R, RStudio and these packages:\n\ninstall.packages(c(\"Ecdat\", \"devtools\", \"janitor\", \"plm\", \"pwt9\",\n    \"quarto\", \"renv\", \"rio\", \"shiny\", \"targets\", \"tarchetypes\",\n    \"testthat\", \"tidyverse\", \"usethis\"))\n\nThe course will be very, very hands-on. I’ll give general hints and steps, and ask you to do stuff. It will not always be 100% simple and obvious, and you will need to also think a bit by yourself. I’ll help of course, so don’t worry. The idea is to put you in the shoes of a real data scientist that gets asked at 9 in the morning to come up with a solution to a problem by COB. In 99% of the cases, you will never have encountered that problem ever, as it will be very specific to the company you’re working at. Google and Stackoverflow will be your only friends in these moments.\nThe beginning of this course will likely be the toughest part, especially if you’re not familiar with R. I will need to bring you up to speed in 6 hours. Only after can we actually start talking about RAPs. What’s important is to never give up and work together with me."
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Grading",
    "text": "Grading\nThe way grading works in this course is as follows: during lecture hours you will follow along. At home, you’ll be working on setting up your own pipeline. For this, choose a dataset that ideally would need some cleaning and/or tweaking to be usable. We are going first to learn how to package this dataset alongside some functions to make it clean. If time allows, I’ll leave some time during lecture hours for you to work on it and ask me and your colleagues for help. At the end of the semester, I will need to download your code and get it running. The less effort this takes me, the better your score. Here is a tentative breakdown:\n\nCode is on github.com and I can pull it: 2 points;\nData and functions to run pipeline are in a tested, documented package? 3 points;\nI don’t need to do anything to load data: 5 points;\nI can download and install your pipeline’s dependencies in one command line: 5 points;\nI can run your pipeline in one command line: 5 points\nExtra points: pipeline is dockerized and uses github actions to run? 5 points\n\nThe way to fail this class is to write an undocumented script that only runs on your machine and expect me to debug it to get it to run."
  },
  {
    "objectID": "index.html#jargon",
    "href": "index.html#jargon",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Jargon",
    "text": "Jargon\nThere’s some jargon that is helpful to know when working with R. Here’s a non-exhaustive list to get you started:\n\nCRAN: the Comprehensive R Archive Network. This is a curated online repository of packages and R installers. When you type install.packages(\"package_name\") in an R console, the package gets downloaded from there;\nLibrary: the collection of R packages installed on your machine;\nR console: the program where the R interpreter runs;\nPosit/RStudio: Posit (named RStudio in the past) are the makers of the RStudio IDE and of the tidyverse collection of packages;\ntidyverse: a collection of packages created by Posit that offer a common language and syntax to perform any task required for data science — from reading in data, to cleaning data, up to machine learning and visualisation;\nbase R: refers to a vanilla installation (and vanilla capabilities) of R. Often used to contrast a tidyverse specific approach to a problem (for example, using base R’s lapply() in constrast to the tidyverse purrr::map()).\npackage::function(): Functions can be accessed in several ways in R, either by loading an entire package at the start of a script with library(dplyr) or by using dplyr::select().\nFunction factory (sometimes adverb): a function that returns a function.\nVariable: the variable of a function (as in x in f(x)) or the variable from statistical modeling (synonym of feature)\n<- vs =: in practice, you can use <- and = interchangeably. I prefer <-, but feel free to use = if you wish."
  },
  {
    "objectID": "index.html#further-reading",
    "href": "index.html#further-reading",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "Further reading",
    "text": "Further reading\n\nAn Introduction to R (from the R team themselves)\nWhat is CRAN?\nThe R Inferno\nReproducible Analytical Pipelines (RAP)"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Building Reproducible Analytical Pipelines",
    "section": "License",
    "text": "License\nThis course is licensed under the WTFPL."
  },
  {
    "objectID": "02-intro_R.html#reading-in-data-with-r",
    "href": "02-intro_R.html#reading-in-data-with-r",
    "title": "1  Introduction to R",
    "section": "1.1 Reading in data with R",
    "text": "1.1 Reading in data with R\nYour first job is to actually get the following datasets into an R session.\nFirst install the {rio} package (if you don’t have it already), then download the following datasets:\n\nmtcars.csv\nmtcars.dta\nmtcars.sas7bdat\nmulti.xlsx\n\nAlso download the following 4 csv files and put them in a directory called unemployment:\n\nunemp_2013.csv\nunemp_2014.csv\nunemp_2015.csv\nunemp_2016.csv\n\nFinally, download this one as well, but put it in a folder called problem:\n\nmtcars.csv\n\nand take a look at chapter 3 of my other book, Modern R with the {tidyverse} and follow along. This will teach you to import and export data.\n{rio} is some kind of wrapper around many packages. You can keep using {rio}, but it is also a good idea to know which packages are used under the hood by {rio}. For this, you can take a look at this vignette.\nIf you need to import very large datasets (potentially several GBs), you might want to look at packages like {vroom} (this benchmark shows a 1.5G csv file getting imported in seconds by {vroom}. For even larger files, take a look at {arrow} here. This package is able to efficiently read very large files (csv, json, parquet and feather formats)."
  },
  {
    "objectID": "02-intro_R.html#a-little-aside-on-pipes",
    "href": "02-intro_R.html#a-little-aside-on-pipes",
    "title": "1  Introduction to R",
    "section": "1.2 A little aside on pipes",
    "text": "1.2 A little aside on pipes\nSince R version 4.1, a forward pipe |> is included in the standard library of the language. It allows to do this:\n\n4 |>\n  sqrt()\n\n[1] 2\n\n\nBefore R version 4.1, there was already a forward pipe, introduced with the {magrittr} package (and automatically loaded by many other packages from the tidyverse, like {dplyr}):\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n4 %>%\n  sqrt()\n\n[1] 2\n\n\nBoth expressions above are equivalent to sqrt(4). You will see why this is useful very soon. For now, just know this exists and try to get used to it."
  },
  {
    "objectID": "02-intro_R.html#exploring-and-cleaning-data-with-r",
    "href": "02-intro_R.html#exploring-and-cleaning-data-with-r",
    "title": "1  Introduction to R",
    "section": "1.3 Exploring and cleaning data with R",
    "text": "1.3 Exploring and cleaning data with R\nTake a look at chapter 4 of my other book, ideally you should study the entirety of the chapter, but for our purposes you should really focus on sections 4.3, 4.4, 4.5.3, 4.5.4, (optionally 4.7) and 4.8."
  },
  {
    "objectID": "02-intro_R.html#data-visualization",
    "href": "02-intro_R.html#data-visualization",
    "title": "1  Introduction to R",
    "section": "1.4 Data visualization",
    "text": "1.4 Data visualization\nWe’re not going to focus on visualization due to lack of time. If you need to create graphs, read chapter 5."
  },
  {
    "objectID": "02-intro_R.html#further-reading",
    "href": "02-intro_R.html#further-reading",
    "title": "1  Introduction to R",
    "section": "1.5 Further reading",
    "text": "1.5 Further reading\nR for Data Science"
  },
  {
    "objectID": "03-functional-programming.html#introduction",
    "href": "03-functional-programming.html#introduction",
    "title": "2  A primer on functional programming",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nFunctional programming is a way of writing programs that relies exclusively on the evalutation of functions. Mathematical functions have a very neat property: for any given input, they ALWAYS return exactly the same output. This is what we want to achieve with the functions that we will write. Functions that always return the same result are called pure, and a language that only allows writing pure functions is called a pure functional programming language. R is not a pure functional programming language, so we have to be careful not to write impure functions that manipulate the global state.\nBut what is state? Run the following code in your console:\n\nls()\n\nThis will list every object defined in the global environment. Now run the following line:\n\nx <- 1\n\nand then ls() again. x should now be listed alongside the other objects. You just manipulated the state of your current R session. Now if you run something like:\n\nx + 1\n\nThis will produce 2. We want to avoid pipelines that depend on some definition of some global variable somewhere, which could be subject to change, because this could mean that 2 different runs of the same pipeline could produce 2 different results. Notice that I used the verb avoid in the sentence before. This is sometimes not possible to avoid. Such situations have to be carefully documented and controlled.\nAs a more realistic example, imagine that within the pipeline you set up, some random numbers are generated. For example, to generate 10 random draws from a normal distribution:\n\nrnorm(n = 10)\n\n [1] -1.444674478 -0.487202309 -0.466214409 -0.318374417 -0.104245822\n [6]  0.291212491 -0.008407899  1.229025714 -0.990083916 -0.504895535\n\n\nEach time you run this line, you will get another set of 10 random numbers. This is obviously a good thing in interactive data analysis, but much less so when running a pipeline programmatically. R provides a way to fix the random seed, which will make sure you always get the same random numbers:\n\nset.seed(1234)\nrnorm(n = 10)\n\n [1] -1.2070657  0.2774292  1.0844412 -2.3456977  0.4291247  0.5060559\n [7] -0.5747400 -0.5466319 -0.5644520 -0.8900378\n\n\nBut set.seed() only works for one call, so you must call it again if you need the random numbers again:\n\nset.seed(1234)\nrnorm(10)\n\n [1] -1.2070657  0.2774292  1.0844412 -2.3456977  0.4291247  0.5060559\n [7] -0.5747400 -0.5466319 -0.5644520 -0.8900378\n\nrnorm(10)\n\n [1] -0.47719270 -0.99838644 -0.77625389  0.06445882  0.95949406 -0.11028549\n [7] -0.51100951 -0.91119542 -0.83717168  2.41583518\n\nset.seed(1234)\nrnorm(10)\n\n [1] -1.2070657  0.2774292  1.0844412 -2.3456977  0.4291247  0.5060559\n [7] -0.5747400 -0.5466319 -0.5644520 -0.8900378\n\n\nThe problem with set.seed() is that you only partially solve the problem of rnorm() not being pure; this is because while rnorm() now does return the same output for the same input, this only works if you manipulate the state of your program to change the seed beforehand. Ideally, we would like to have a pure version of rnorm(), which would be self-contained and not depend on the value of the seed defined in the global environment. There is a package developped by Posit (the makers of RStudio and the packages from the tidyverse), called {withr} which allows to rewrite our functions in a pure way. {withr} has several functions, all starting with with_ that allow users to run code with some temporary defined variables, without altering the global environment. For example, it is possible to run a rnorm() with a seed, using withr::with_seed():\n\nlibrary(withr)\n\nwith_seed(seed = 1234, {\n  rnorm(10)\n})\n\n [1] -1.2070657  0.2774292  1.0844412 -2.3456977  0.4291247  0.5060559\n [7] -0.5747400 -0.5466319 -0.5644520 -0.8900378\n\n\nBut ideally you’d want to go a step further and define a new function that is pure. To turn an impure function into a pure function, you usually only need to add some arguments to it. This is how we would create a pure_rnorm() function:\n\npure_rnorm <- function(..., seed){\n\n  with_seed(seed, rnorm(...))\n}\n\npure_rnorm(10, seed = 1234)\n\n [1] -1.2070657  0.2774292  1.0844412 -2.3456977  0.4291247  0.5060559\n [7] -0.5747400 -0.5466319 -0.5644520 -0.8900378\n\n\npure_rnorm() is now self-contained, and does not pollute the global environment. We’re going to learn how to write functions in just a bit, so don’t worry if the code above does not make sense yet.\n\n\n\n\n\n\n\nA very practical consequence of using functional programming is that loops are not used, because loops are imperative and imperative programming is all about manipulating state. However, there are situations where loops are more efficient than the alternative (in R at least). So we will still learn and use them, but only when absolutely necessary, and we will always encapsulate a loop inside a function. Just like with the example above, this ensures that we have a pure, self-contained function that we can reason about easily. What I mean by this, is that loops are not always very easy to decipher. The concept of loops is simple enough: take this instruction, and repeat it N times. But in practice, if you’re reading code, it is not possible to understand what a loop is doing at first glance. There are only two solutions in this case:\n\nyou’re lucky and there are comments that explain what the loop is doing;\nyou have to let the loop run either in your head or in a console with some examples to really understand whit is going on.\n\nFor example, consider the following code:\n\nsuppressPackageStartupMessages(library(dplyr))\n\ndata(starwars)\n\nsum_humans <- 0\nsum_others <- 0\nn_humans <- 0\nn_others <- 0\n\nfor(i in seq_along(1:nrow(starwars))){\n\n  if(!is.na(unlist(starwars[i, \"species\"])) &\n     unlist(starwars[i, \"species\"]) == \"Human\"){\n    if(!is.na(unlist(starwars[i, \"height\"]))){\n      sum_humans <- sum_humans + unlist(starwars[i, \"height\"])\n      n_humans <- n_humans + 1\n    } else {\n\n      0\n\n    }\n\n  } else {\n    if(!is.na(unlist(starwars[i, \"height\"]))){\n      sum_others <- sum_others + unlist(starwars[i, \"height\"])\n      n_others <- n_others + 1\n    } else {\n      0\n    }\n  }\n}\n\nmean_height_humans <- sum_humans/n_humans\nmean_height_others <- sum_others/n_others\n\nWhat this does is not immediately obvious. The only hint you get are the two last lines, where you can read that we compute the average height for humans and non-humans in the sample. And this code could look a lot worse, because I am using functions like is.na() to test if a value is NA or not, and I’m using unlist() as well. If you compare this mess to a functional approach, I hope that I can stop my diatribe against imperative style programming here:\n\nstarwars %>%\n  group_by(is_human = species == \"Human\") %>%\n  summarise(mean_height = mean(height, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  is_human mean_height\n  <lgl>          <dbl>\n1 FALSE           172.\n2 TRUE            177.\n3 NA              181.\n\n\nNot only is this shorter, it doesn’t even need any comments to explain what’s going on. If you’re using functions with explicit names, the code becomes self-explanatory.\nThe other advantage of a functional (also called declarative) programming style is that you get function composition for free. Function composition is an operation that takes two functions g and f and returns a new function h such that \\(h(x) = g(f(x))\\). Formally:\nh = g ∘ f such that h(x) = g(f(x))\n∘ is the composition operator. You can read g ∘ f as g after f. When using functional programming, you can compose functions very easily, simply by using |> or %>%:\n\nh <- f |> g\n\nf |> g can be read as f then g, which is equivalent to g after f. Function composition might not seem like a big deal, but it actually is. If we structure our programs in this way, as a sequence of function calls, we get many benefits. Functions are easy to test, document, maintain, share and can be composed. This allows us to very succintly express complex workflows:\n\nstarwars %>%\n  filter(skin_color == \"light\") %>%\n  select(species, sex, mass) %>%\n  group_by(sex, species) %>%\n  summarise(\n    total_individuals = n(),\n    min_mass = min(mass, na.rm = TRUE),\n    mean_mass = mean(mass, na.rm = TRUE),\n    sd_mass = sd(mass, na.rm = TRUE),\n    max_mass = max(mass, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  select(-species) %>%\n  tidyr::pivot_longer(-sex, names_to = \"statistic\", values_to = \"value\")\n\n# A tibble: 10 × 3\n   sex    statistic         value\n   <chr>  <chr>             <dbl>\n 1 female total_individuals   6  \n 2 female min_mass           45  \n 3 female mean_mass          56.3\n 4 female sd_mass            16.3\n 5 female max_mass           75  \n 6 male   total_individuals   5  \n 7 male   min_mass           79  \n 8 male   mean_mass          90.5\n 9 male   sd_mass            19.8\n10 male   max_mass          120  \n\n\nNeedless to say, writing this in an imperative approach would be quite complicated.\nAnother consequence of using functional programming is that our code will live in plain text files, and not in Jupyter (or equivalent) notebooks. Not only does imperative code have state, but notebooks themselves have a (hidden) state. You should avoid notebooks at all costs, even for experimenting."
  },
  {
    "objectID": "03-functional-programming.html#defining-your-own-functions",
    "href": "03-functional-programming.html#defining-your-own-functions",
    "title": "2  A primer on functional programming",
    "section": "2.2 Defining your own functions",
    "text": "2.2 Defining your own functions\nLet’s first learn about actually writing functions. Read chapter 7 of my other book.\nThe most important concepts for this course are discussed in the following sections:\n\nfunctions that take functions as arguments (section 7.4)\nfunctions that take data (and the data’s columns) as arguments (section 7.6);"
  },
  {
    "objectID": "03-functional-programming.html#functional-programming",
    "href": "03-functional-programming.html#functional-programming",
    "title": "2  A primer on functional programming",
    "section": "2.3 Functional programming",
    "text": "2.3 Functional programming\nYou should ideally work through the whole of chapter 7, and then tackle chapter 8. What’s important there are:\n\npurrr::map(), purrr::reduce() (sections 8.3.1 and 8.3.2)\nAnd list based workflows (section 8.4)"
  },
  {
    "objectID": "03-functional-programming.html#further-reading",
    "href": "03-functional-programming.html#further-reading",
    "title": "2  A primer on functional programming",
    "section": "2.4 Further reading",
    "text": "2.4 Further reading\n\nCleaner R Code with Functional Programming\nFunctional Programming (Chapter from Advanced R)\nWhy you should(n’t) care about Monads if you’re an R programmer\nSome learnings from functional programming you can use to write safer programs"
  },
  {
    "objectID": "04-git.html#introduction",
    "href": "04-git.html#introduction",
    "title": "3  Git",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nGit is a software for version control. Version control is absolutely essential in software engineering, or when setting up a RAP. If you don’t install a version control system such as Git, don’t even start trying to set up a RAP. But what does a version control system like Git actually do? The basic workflow of Git is as follows: you start by setting up a repository for a project. On your computer, this is nothing more than a folder with your scripts in it. However, if you’re using Git to keep track of what’s inside that folder, there will be a hidden .git folder with a bunch of files in it. You can forget about that folder, this is for Git’s own internal needs. What matters, is that when you make changes to your files, you can first commit these changes, and then push them back to a repository. Collaborators can copy this repository and synchronize their files saved on their computers with your changes. Your collaborators can then also work on the files, then commit and push the changes to the repository as well.\nYou can then pull back these changes onto your computer, add more code, commit, push, etc… Git makes it easy to collaborate on projects either with other people, or with future you. It is possible to roll back to previous versions of your code base, you can create new branches of your project to test new features (without affecting the main branch of your code), collaborators can submit patches that you can review and merge, and and and…\nIn my experience, learning git is one of the most difficult things there is for students. And this is because Git solves a complex problem, and there is no easy way to solve a complex problem. But I would however say that Git is not unnescessarily complex. So buckle up, because this chapter is not going to be easy.\nGit is incredibly powerful, and absolutely essential in our line of work, it is simply not p ossible to not know at least some basics of Git. And this is what we’re going to do, learn the basics, it’ll keep us plenty busy already.\nBut for now, let’s pause for a brief moment and watch this video that explains in 2 minutes the general idea of Git.\nLet’s get started.\nYou might have heard of github.com: this is a website that allows programmers to set up repositories on which they can host their code. The way to interact with github.com is via Git; but there are many other website like github.com, such as gitlab.com and bitbucket.com.\nFor this course, you should create an account on github.com. This should be easy enough. Then you should install Git on your computer."
  },
  {
    "objectID": "04-git.html#installing-git",
    "href": "04-git.html#installing-git",
    "title": "3  Git",
    "section": "3.2 Installing Git",
    "text": "3.2 Installing Git\nInstalling Git is not hard; it installs like any piece of software on your computer. If you’re running a Linux distribution, chances are you already have Git installed. To check if it’s already installed on a Linux system, open a terminal and type which git. If a path gets returned, like usr/bin/gin, congratulations, it’s installed, if the command returns nothing you’ll have to install it. On Ubuntu, type sudo apt-get install git and just wait a bit. If you’re using macOS or Windows, you will need to install it manually. For Windows, download the installer from here, and for macOS from here; you’ll see that there are several ways of installing it on macOS, if you’ve never heard of homebrew or macports then install the binary package from https://sourceforge.net/projects/git-osx-installer/."
  },
  {
    "objectID": "04-git.html#setting-up-a-repo",
    "href": "04-git.html#setting-up-a-repo",
    "title": "3  Git",
    "section": "3.3 Setting up a repo",
    "text": "3.3 Setting up a repo\nOk so now that Git is installed, we can actually start using it. First, let’s start by creating a new repository on github.com. As I’ve mentioned in the introductory paragraph, Git will allow you to interact with github.com, and you’ll see in what ways soon enough. For now, login to your github.com account, and create a new repository by clicking on the ‘plus’ sign in the top right corner of your profile page and then choose ‘New repository’:\n\n\n\n\n\n\n\nIn the next screen, choose a nice name for your repository and ignore the other options, they’re not important for now. Then click on ‘Create repository’:\n\n\n\n\n\n\n\nOk, we’re almost done with the easy part. The next screen tells us we can start interacting with the repository. For this, we’re first going to click on ‘README’:\n\n\n\n\n\n\n\nThis will add a README file that we can also edit from github.com directly:\n\n\n\n\n\n\n\nAdd some lines to the file, and then click on ‘Commit new file’. You’ll end up on the main page of your freshly created repository. We are now done with setting up the repository on github.com. We can now clone the repository onto our machines. For this, click on ‘Code’, then ‘SSH’ and then on the copy icon:\n\n\n\n\n\n\n\nNow, to make things easier on you, we’re going to use Rstudio as an interface for Git. But you should know that Git can be used independently from a terminal application on Linux or macOS, or from Git Bash on Windows, and you should definitely get familiar with the Linux/macOS command line at some point if you wish to become a data scientist. This is because most servers, if not all, that you are going to interact with in your career are running some flavour of Linux. But since the Linux command line is outside the scope of this course, we’ll use Rstudio instead (well, we’ll use it as much as we can, because at some point it won’t be enough and have to use the terminal instead anyways…)."
  },
  {
    "objectID": "04-git.html#cloning-the-repository-onto-your-computer",
    "href": "04-git.html#cloning-the-repository-onto-your-computer",
    "title": "3  Git",
    "section": "3.4 Cloning the repository onto your computer",
    "text": "3.4 Cloning the repository onto your computer\nStart Rstudio and click on ‘new project’ and then ‘Version Control’:\n\n\n\n\n\n\n\nThen choose ‘Git’:\n\n\n\n\n\n\n\nThen paste the link from before into the ‘Repository URL’ field, the ‘project directory name’ will fill out automatically, choose where to save the repository in your computer, click on ‘Open in new session’ and then on ‘Create Project’:\n\n\n\n\n\n\n\nA new Rstudio window should open. There are several things that you should pay attention to now:\n\n\n\n\n\n\n\nIcon (1) indicates that this project is git-enabled so to speak. (2) shows you that Rstudio is open inside the example_repo (or whatever you named your repo to) project, and (3) shows you the actual repository that was downloaded from github.com at the path you chose before. You will also see the README file that we created before."
  },
  {
    "objectID": "04-git.html#your-first-commit",
    "href": "04-git.html#your-first-commit",
    "title": "3  Git",
    "section": "3.5 Your first commit",
    "text": "3.5 Your first commit\nLet’s now create a simple script and add some lines of code to it, and save it. Check out the Git tab now, you should see your script there, alongside a ? icon:\n\n\n\n\n\n\n\nWe are now ready to commit the file, but first let’s check out what actually changed. If you click on Diff, a new window will open with the different files that changed since last time:\n\n\n\n\n\n\n\nIcon (1) shows you the list of files that changed. We only created the file called my_script.R, but two other files are listed as well. These files are automatically generated when starting a new project. .gitignore lists files and folders that Git should not track, meaning, any change that will affect these files will be ignored by Git. This means that these files will also not be uploaded to github.com when committing. The file ending with the .Rproj extension is a RStudio specific file, which simply defines some variables that help RStudio start your project. What matters here is that the files you changed are listed, and that you saved them. You can double check that you actually correctly saved your files by looking at (2), which lists the lines that were added (added lines will be highlighted in green, deleted lines in red). In (3) you can write a commit message. This message should be informative enough that a coworker, or future you, can read through them and have a rough idea of what changed. Best practice is to commit often and early, and try to have one commit per change (per file for example, or per function within that file) that you make. Let’s write something like: “Started project: first graph done” as the commit message. We’re almost done: now let’s stage the files for this commit. This means that we can choose which files should actually be included in this commit. You can only stage one file, several files, or all files. Since this is our first commit, let’s stage everything we’ve got, by simply clicking on the checkboxes below the column Staged in (1).\n\n\n\n\n\n\n\nThe status of the files now changed: they’ve been added for this commit. We can now click on the Commit button. Now these changes have been committed there are no unstaged files anymore. We have two options at this point: we can continue working, and then do another commit, or we can push our changes to github.com. Committing without pushing does not make our changes available to our colleagues, but because we committed them, we can recover our changes. For example, if I continue working on my file and remove some lines by mistake, I can recover them (I’ll show you how to do this later on). But it is a much better idea to push our commit now. This makes our changes available to colleagues (who need to pull the changes from github.com) and should our computer spontaneously combust, at least or work is now securely saved on github.com. So let’s Push:\n\n\n\n\n\n\n\nOoooooops! Something’s wrong! Apparently, we do not have access rights to the repo? This can sound weird, because after all, we created the repo with our account and then cloned it. So what’s going on? Well, remember that anyone can clone a public repository, but only authorized people can push changes to it. So at this stage, the Git software (that we’re using through RStudio) has no clue who you are. Git simply doesn’t know that your the admin of the repository. You need to provide a way for Git to know by logging in. And the way you login is through a so-called ssh key.\nNow if you thought that Git was confusing, I’m sorry to say that what’s coming confuses students in general even more. Ok so what’s a ssh key, and why does Git need it? An ssh key is actually a misnomer, because we should really be talking about a pair of keys. The idea is that you generated two files on the computer that you need to access github.com from. One of these keys will be a public key, the other a private key. The private key will be a file usually called id_rsa without any extension, while the public key will be called the same, but with a .pub extension, so id_rsa.pub (we will generate these two files using RStudio in a bit). What you do is that you give the public key to github.com, but you keep your private key on your machine. Never, ever, upload or share your private key with anyone! It’s called private for a reason. Once github.com has your public key, each time you want to push to github.com, what happens is that the public key is checked against your private key. If they match, github.com knows that you are the person you claim to be, and will allow you to push to the repository. If not you will get the error from before.\nSo let’s now generate an ssh key pair. For this, go to Tools > Global Options > Git/Svn, and then click on the Create RSA Key...\n\n\n\n\n\n\n\nIcon (1) shows you the path where the keys will be saved. This is only useful if you have reasons to worry that your private key might be compromised, but without physical access to your machine, an attacker would have a lot of trouble retrieving it (if you keep your OS updated…). Finally click on Create:\n\n\n\n\n\n\n\nOk so now that you have generated these keys, let’s copy the public key in our clipboard (because we need to paste the key into github.com). You should be able to find this key from RStudio. Go back to Tools > Global Options > Git/Svn, and then click on View public key:\n\n\n\n\n\n\n\nA new window will open showing you your public key. You can now copy and paste it into github.com. For this, first go to your profile, then Settings then SSH and GPG keys:\n\n\n\n\n\n\n\nThen, on the new screen click on New SSH key:\n\n\n\n\n\n\n\nYou can now add your key. Add a title, for example home for your home computer, or work for your work laptop. Paste the key from RStudio into the field (2), and then click on Add SSH key:\n\n\n\n\n\n\n\nOk, now that github.com has your public key, you can now push your commits without any error. Go back to RStudio, to the Git tab and click on Push:\n\n\n\n\n\n\n\nA new window will open, this time showing you that the upload went through:\n\n\n\n\n\n\n\nYou will need to add one public key per computer you use on github.com. In the past, it was possible to push your commits by providing a password each time. This was not secure enough however, so now the only way to to push commits is via ssh key pairs. This concept is quite important: whatever service you use, even if your company has a private Git server instance, you will need to provide the public key to the central server. All of this ssh key pair business IS NOT specific to github.com, so make sure that you understand this well, because sooner rather later, you will need to provide another public key, either because you work from several computers or because the your first job will have it’s own Git instance.\nOk so now you have an account on github.com, and know how to set up a repo and push code to it. This is already quite useful, because it allows you and future you to collaborate. What I mean by this is that if in two or three months you need to go back to some previous version of your code this is now possible. Let’s try it out; change the file by adding some lines to it, commit your changes and push again. Remember to use a commit message that explain what you did. Once you’re done, go back to the Git tab of Rstudio, and click on the History button (the icon is a clock):\n\n\n\n\n\n\n\nAs you can see from the picture above, clicking on History shows every commit since the beginning of the repo. It also shows you who pushed that particular commit, and when. For now, you will only see your name. At (1) you see the lines I’ve added. These are reflected, in green, in the History window. If I had removed some lines, these would have been highlighted in red in the same window. (4) shows you the only commit history. There’s not much for now, but for projects that have been ongoing for some time, this can get quite long! Finally, (5) shows many interesting details. As before, who pushed the commit, when, the commit message (under Subject), and finally the SHA. This is a unique sequence of characters that identifies the commit. If you select another commit, you will notice that its SHA is different:\n\n\n\n\n\n\n\nThe SHA identifier (called a hash) is what we’re going to use to revert to a previous state of the code base. But because this is a bit advanced, there is no way of doing it from RStudio. You will need to open a terminal and use Git from there. On Windows, go to the folder of your project, right-click on some white space and select Git Bash Here:\n\n\n\n\n\n\n\nA similar approach can be used for most Linux distributions (but simply open a terminal, Git Bash is Windows only), and you can apparently do something similar on macOS, but first need to active the required service as explained here. You can also simply open a terminal and navigate to the right folder using cd.1\nOnce the terminal is opened, follow along but by adapting the paths to your computer:\n\n# The first line changes the working directory to my github repo on my computer\n# If you did not open the terminal inside the folder as explained above, you need\n# adapt the path.\n\ncd ~/six_to/example_repo  # example_repo is the folder where I cloned the repo\nls # List all the files in the directory\n\nListing the files inside the folder confirms that I’m in the right spot. Something else you could do here is try out some git commands, for example, git log:\n\ngit log\n\n## commit bd7daf0dafb12c0a19ba65f85b54834a02f7d150\n## Author: Bruno Rodrigues <bruno@brodrigues.co>\n## Date:   Mon Oct 17 14:38:59 2022 +0200\n## \n##     added some more lines\n## \n## commit 95c26ed4dffd8fc40503f25ddc11af7de5c586c0\n## Author: Bruno Rodrigues <bruno@brodrigues.co>\n## Date:   Sat Oct 15 12:52:43 2022 +0200\n## \n##     Started project: first graph done\n## \n## commit d9cff70ff71241ed8514cb65d97e669b0bbdf0f6\n## Author: Bruno Rodrigues <brodriguesco@protonmail.com>\n## Date:   Thu Oct 13 22:12:06 2022 +0200\n## \n##     Create README.md\n\ngit log returns the same stuff as the History button of the Git pane inside RStudio. You see the commit hash, the name of the author and when the commit was pushed. At this stage, we have two options. We could “go back in time”, but just look around, and then go back to where the repository stands currently. Or we could essentially go back in time, and stay there, meaning, we actually revert the code base back. Let’s try the first option, let’s just take a look around at the code base at a particular point in time. Copy the hash of a previous commit. With the hash in your clipboard, use the git checkout command to go back to this commit:\n\ngit checkout 95c26ed4dffd8f\n\nYou will see an output similar to this:\nNote: switching to '95c26ed4dffd8f'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental changes\nand commit them, and you can discard any commits you make in this state without\nimpacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may do so\n(now or later) by using -c with the switch command. Example:\n\n  git switch -c <new-branch-name>\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at 95c26ed Started project: first graph done\n\nWhen checking out a commit, you are in detached HEAD state. I won’t go into specifics, but what this means is that anything you do here, won’t get saved, unless you specifically create a new branch for it. A Git repository is composed of branches. The branche you’re currently working on should be called main or master. You can create new branches, and continue working on these other branches, without affecting the master branch. This allows to explore new ideas and experiment. If this turns out to be fruitful, you can merge the experimental branch back into master. We are not going to explore branches in this course, so you’ll have to read about it on your own. But don’t worry, branches are not that difficult to grok.\nTake a look at the script file now, you will see that the lines you added are now missing (the following line only works on Linux, macOS, or inside a Git Bash terminal on Windows. cat is a command line program that prints the contents of a text file to a terminal):\n\ncat my_script.R\n\nOnce you’re done taking your tour, go back to the main (or master) branch by running:\n\ngit checkout main\n\nOk, so how do we actually go back to a previous state? For this, use git revert. But unlike git checkout, you don’t use the hash of the commit you want to go back to. Instead, you need to use the hash of the commit you want to “cancel”. For example, imagine that my commit history looks like this:\n## commit bd7daf0dafb12c0a19ba65f85b54834a02f7d150\n## Author: Bruno Rodrigues <bruno@brodrigues.co>\n## Date:   Mon Oct 17 14:38:59 2022 +0200\n## \n##     added some more lines\n## \n## commit 95c26ed4dffd8fc40503f25ddc11af7de5c586c0\n## Author: Bruno Rodrigues <bruno@brodrigues.co>\n## Date:   Sat Oct 15 12:52:43 2022 +0200\n## \n##     Started project: first graph done\n## \n## commit d9cff70ff71241ed8514cb65d97e669b0bbdf0f6\n## Author: Bruno Rodrigues <brodriguesco@protonmail.com>\n## Date:   Thu Oct 13 22:12:06 2022 +0200\n## \n##     Create README.md\n\nand let’s suppose I want to go back to commit 95c26ed4dffd8fc (so my second commit). What I need to do is essentially cancel commit bd7daf0dafb1, which comes after commit 95c26ed4dffd8fc (look at the dates: commit 95c26ed4dffd8fc was made on October 15th and commit bd7daf0dafb1 was made on October 17th). So I need to revert commit bd7daf0dafb1. And that’s what we’re going to do:\n\ngit revert bd7daf0dafb1\n\nThis opens a text editor inside your terminal. Here you can add a commit message or just keep the one that was added by default. Let’s just keep it and quit the text editor. Unfortunately, this is not very use friendly, but to quit the editor type :q. (The editor that was opened is vim, a very powerful terminal editor, but with a very steep learning curve.) Now you’re back inside your terminal. Type git log and you will see a new commit (that you have yet to push), which essentially cancels the commit bd7daf0dafb1. You can now push this; for pushing this one, let’s stay inside the terminal and use the following command:\n\ngit push origin main\n\norigin main: origin here refers to the remote repository, so to github.com, and main to the main branch.\nOk, we’re doing with the basics. Let’s now see how we can contribute to some repository."
  },
  {
    "objectID": "04-git.html#collaborating",
    "href": "04-git.html#collaborating",
    "title": "3  Git",
    "section": "3.6 Collaborating",
    "text": "3.6 Collaborating\nGithub (and similar services) allow you to collaborate with people. There are two ways of achieving this. You can invite people to work with you on the same project, by giving them writing rights to the repository. This is what we are going to cover in this section. The other way to collaborate is to let strangers fork your repository (make a copy of it on github.com); they can then work on their copy of the project independently from you. If they want to submit patches to you, they can do so by doing a so-called pull request. This workflow is quite different from what we’ll see here and will be discussed in the next section.\nSo for this section you will need to form teams of at least 2 people. One of you will invite the other to collaborate by going on github.com and then following the instructions in the picture below:\n\n\n\n\n\n\n\nType the username of your colleague to find him/her. In my case I’m inviting my good friend David Solito:\n\n\n\n\n\n\n\nDavid now essentially owns the repository as well! So he can contribute to it, just like me. Now, let’s suppose that I continue working on my end, and don’t coordinate with David. After all, this is a post-covid world, so David might be working asynchronously from home, and maybe he lives in an entire different time zone completely! What’s important to realize, is that unlike other ways of collaborating online (for example with an office suite), you do not need to coordinate to collaborate with Git.\nThe file should look like this (yours might be different, it doesn’t matter):\n\ndata(mtcars)\n\nplot(mtcars$mpg, mtcars$hp)\n\nI’m going to change it to this:\n\nlibrary(ggplot2)\n\ndata(mtcars)\n\nggplot(data = mtcars) +\n  geom_point(aes(y = hp, x = mpg))\n\n\n\n\nThe only thing I did was change from the base plotting functions to {ggplot2}. Since you guys formed groups, please work independently on the repository. Go crazy, change some lines, add lines, remove lines, or add new files with new things. Just work as normal, and commit and push your changes and see what happens.\nSo let’s commit and push. You can do it from RStudio or from the command line/Git Bash. This is what I’ll be doing from now on, but feel free to continue using Git through RStudio:\n\ngit add . # This adds every file I've changed to this next commit\ngit commit -am \"Remade plot with ggplot2\" # git commit is the command to create the commit. The -am flag means: 'a' stands for all, as in 'adding all files to the commit', so it's actually redundant with the previous line, but I use it out of habit, and 'm' specifies that we want to add a message\ngit push origin main # This pushes the commit to the repository on github.com\n\nAnd this is what happens:\n➤ git push origin main\n\n   To github.com:b-rodrigues/example_repo.git\n    ! [rejected]        main -> main (fetch first)\n   error: failed to push some refs to 'github.com:b-rodrigues/example_repo.git'\n   hint: Updates were rejected because the remote contains work that you do\n   hint: not have locally. This is usually caused by another repository pushing\n   hint: to the same ref. You may want to first integrate the remote changes\n   hint: (e.g., 'git pull ...') before pushing again.\n   hint: See the 'Note about fast-forwards' in 'git push --help' for details.\nWhat this all means is that David already pushed some changes while I was working on the project as well. It says so very cleary Updates were rejected because the remote contains work that you do not have locally. Git tells us that we first need to pull (download, if you will) the changes to our own computer to integrate the changes, and then we can push again.\nAt this point, if we want, we can first go to github.com and see the commit history there to see what David did. Go to your repo, and click on the commit history icon:\n\n\n\n\n\n\n\nDoing so will list the commit history, as currently on github.com:\n\n\n\n\n\n\n\nWhile I was working, David pushed 2 commits to the repository. If you compare to your local history, using git log you will see that these commits are not there, but instead, however many commits you did (this will not be the case for all of you; whoever of you pushed first will not see any difference between the local and remote repository). Let’s see how it looks for me:\n\ngit log\n\ncommit d2ab909fc679a5661fc3c49c7ac549a2764c539e (HEAD -> main)\nAuthor: Bruno Rodrigues <bruno@brodrigues.co>\nDate:   Tue Oct 18 09:28:10 2022 +0200\n\n    Remade plot with ggplot2\n\ncommit e66c68cc8b58831004d1c9433b2223503d718e1c (origin/main, origin/HEAD)\nAuthor: Bruno Rodrigues <bruno@brodrigues.co>\nDate:   Mon Oct 17 17:33:33 2022 +0200\n\n    Revert \"added some more lines\"\n    \n    This reverts commit bd7daf0dafb12c0a19ba65f85b54834a02f7d150.\n\ncommit bd7daf0dafb12c0a19ba65f85b54834a02f7d150\nAuthor: Bruno Rodrigues <bruno@brodrigues.co>\nDate:   Mon Oct 17 14:38:59 2022 +0200\n\n    added some more lines\n\ncommit 95c26ed4dffd8fc40503f25ddc11af7de5c586c0\nAuthor: Bruno Rodrigues <bruno@brodrigues.co>\nDate:   Sat Oct 15 12:52:43 2022 +0200\n\n    Started project: first graph done\n\ncommit d9cff70ff71241ed8514cb65d97e669b0bbdf0f6\nAuthor: Bruno Rodrigues <brodriguesco@protonmail.com>\nDate:   Thu Oct 13 22:12:06 2022 +0200\n\n    Create README.md\n\n\nYep, so none of David’s commits in sight. Let me do what Git told me to do: let’s pull, or download, David’s commits locally:\n\ngit pull --rebase \n\n--rebase is a flag that keeps the commit history linear. There are many different ways you can pull changes, but for our purposes we can focus on --rebase. The other strategies are more advanced, and you might want at some point to take a look at them.\nOnce git pull --rebase is done, we get the following message:\nAuto-merging my_script.R\nCONFLICT (content): Merge conflict in my_script.R\nerror: could not apply d2ab909... Remade plot with ggplot2\nhint: Resolve all conflicts manually, mark them as resolved with\nhint: \"git add/rm <conflicted_files>\", then run \"git rebase --continue\".\nhint: You can instead skip this commit: run \"git rebase --skip\".\nhint: To abort and get back to the state before \"git rebase\", run \"git rebase --abort\".\nCould not apply d2ab909... Remade plot with ggplot2\n\nOnce again, it is important to read what Git is telling us. There is a merge conflict in the my_script.R file. Let’s open it, and see what’s going on:\n\n\n\n\n\n\n\nWe can see two things: the lines that David changed in (1), and the lines I’ve added in (2). This happened because we changed the same lines. Had I added lines instead of changing lines that were already there, the merge would have happened automatically, because there would not have been any conflict. In this case however, Git does not know how to solve the issue: do we keep David’s changes, or mine? Actually, we need to keep both. I’ll keep my version of plot that uses {ggplot2}, but will also keep what David added: he replaced the hp variable by cyl, and added a linear regression as well. Since this seems sensible to me, I will adapt the script in a way that gracefully merges both contributions. So the file looks like this now:\n\n\n\n\n\n\n\nWe can now save, and continue following the hints from Git, namely, adding the changed file to the next commit and then use git rebase --continue:\n\ngit add my_script.R\ngit rebase --continue\n\nThis will once again open the editor in your terminal. Simply close it with :q. Let’s now push:\n\ngit push origin main\n\nand we’re done! Let’s go back to github.com to see the commit history. You can click on the hash to see the details of how the file changed (you can do so from RStudio as well):\n\n\n\n\n\n\n\nIn green, you see lines that were added, and in red, lines that were removed. The lines where the linear model was defined are not impacted, because David wrote them at the bottom of the script, and I did not write anything there:"
  },
  {
    "objectID": "04-git.html#branches",
    "href": "04-git.html#branches",
    "title": "3  Git",
    "section": "3.7 Branches",
    "text": "3.7 Branches\nIt is possible to create new branches and continue working on these branches without impacting the code in the main branch. This is useful if you want to experiment and explore new ideas. The main or master branch can thus be used only to have code that is ready to get shipped and distributed, while you can keep working on a development branch. Let’s create a branch called dev by using the git checkout command, and let’s also add the -b flag to immediately switch to it:\n\n➤ git checkout -b dev \nSwitched to a new branch 'dev'\n\nIt is possible to list the existing branches using git branch:\n\n➤ git branch\n* dev\nmain\n\nAs a little aside, if you’re working inside a terminal instead of RStudio or another GUI application, it might be a good idea to configure your terminal a little bit to do two things:\n\nchange the branch you’re currently on\nshow if some files got changed.\n\nIf you want to keep it simple, following this tutorial should be enough. If you want something more fancy, use this other tutorial. I have not followed either, so I don’t know if they work, but by the looks of it they should, and it should work on both Linux and macOS I believe. If these don’t work, just google for “showing git branch in terminal”. This is entirely optional, and you can use git branch to check which branch you’re currently working on.\nOk so now that we are on the dev branch, let’s change the files a little bit. Change some lines, then commit, then add some new files and commit again. Then push to dev using:\n\n➤ git push origin dev\n\nThis is what you should see on github.com after all is done:\n\n\n\n\n\nThe video below shows you how you can switch between branches and check the commit history of both:\n\n  \n\nLet’s suppose that we are happy with our experiments on the dev branch, and are ready to add them to the master or main branch. For this, checkout the main branch:\n\n➤ git checkout main\n\nYou can now pull from dev. This will update your local main branch with the changes from dev. Depending on what changes you introduced, you might need to solve some conflicts. Try to use the rebase strategy, and then solve the conflict. In my case, the merge didn’t cause an issue:\n\n➤ git pull origin dev\nFrom github.com:b-rodrigues/example_repo\n * branch            dev        -> FETCH_HEAD\nUpdating a9a417f..8b2f04f\nFast-forward\n my_script.R  | 8 +++-----\n new_script.R | 1 +\n 2 files changed, 4 insertions(+), 5 deletions(-)\n create mode 100644 new_script.R\n\nNow if you run git status, this is what you’ll see:\n\n➤ git status\nOn branch main\nYour branch and 'origin/main' have diverged,\nand have 2 and 2 different commits each, respectively.\n  (use \"git pull\" to merge the remote branch into yours)\n\nNow, remember that I’ve pulled from dev into main. But git status complains that the remote main and local main branches have diverged. In these situations, git suggests to pull. This time we’re pulling from main:\n\n➤ git pull\n\nThis will likely result in the following message:\n\nhint: You have divergent branches and need to specify how to reconcile them.\nhint: You can do so by running one of the following commands sometime before\nhint: your next pull:\nhint: \nhint:   git config pull.rebase false  # merge\nhint:   git config pull.rebase true   # rebase\nhint:   git config pull.ff only       # fast-forward only\nhint: \nhint: You can replace \"git config\" with \"git config --global\" to set a default\nhint: preference for all repositories. You can also pass --rebase, --no-rebase,\nhint: or --ff-only on the command line to override the configured default per\nhint: invocation.\nfatal: Need to specify how to reconcile divergent branches.\n\nBecause there are conflicts, I need to specify how the pulling should be done. For this, I’m using once again the rebase flag:\n\n➤ git pull --rebase\nAuto-merging my_script.R\nCONFLICT (content): Merge conflict in my_script.R\nerror: could not apply b240566... lm -> rf\nhint: Resolve all conflicts manually, mark them as resolved with\nhint: \"git add/rm <conflicted_files>\", then run \"git rebase --continue\".\nhint: You can instead skip this commit: run \"git rebase --skip\".\nhint: To abort and get back to the state before \"git rebase\", run \"git rebase --abort\".\nCould not apply b240566... lm -> rf\n\nSo now I have conflicts. This is how the my_script.R file looks like:\n\nlibrary(ggplot2)\nlibrary(randomForest)\n\ndata(mtcars)\n\nggplot(data = mtcars) +\n  geom_point(aes(y = cyl, x = mpg))\n\nrf <- randomForest(hp ~ mpg, data = mtcars)\n\n<<<<<<< HEAD\ndata(iris)\n\nhead(iris)\n=======\nplot(rf)\n>>>>>>> b240566 (lm -> rf)\n\nI need to solve the conflicts, and will do so by keeping the following lines:\n\nlibrary(ggplot2)\nlibrary(randomForest)\n\ndata(mtcars)\n\nggplot(data = mtcars) +\n  geom_point(aes(y = cyl, x = mpg))\n\nrf <- randomForest(hp ~ mpg, data = mtcars)\n\nplot(rf)\n\nLet’s save the script, and call git rebase --continue. You might see something like this:\n\n➤ git rebase --continue\n[detached HEAD 929f4ab] lm -> rf\n 1 file changed, 4 insertions(+), 9 deletions(-)\nAuto-merging new_script.R\nCONFLICT (add/add): Merge conflict in new_script.R\nerror: could not apply 8b2f04f... new file\n\nThere’s another conflict: this time, this is because of the commit 8b2f04f, where I added a new file. This one is easy to solve: I simply want to keep this file, so I simply keep track of it with git add new_script.R and then, once again, call git rebase --continue:\n\n➤ git rebase --continue\n[detached HEAD 20c04f8] new file\n 1 file changed, 4 insertions(+)\nSuccessfully rebased and updated refs/heads/main.\n\nI’m now done and can push to main:\n\n➤ git push origin main\nEnumerating objects: 9, done.\nCounting objects: 100% (9/9), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (6/6), 660 bytes | 660.00 KiB/s, done.\nTotal 6 (delta 2), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (2/2), completed with 1 local object.\nTo github.com:b-rodrigues/example_repo.git\n   83691c2..20c04f8  main -> main\n\nThere are other ways to achieve this. So let’s go back to dev and continue working:\n\n➤ git checkout dev\n\nAdd some lines to my_script.R and then commit and push:\n\n➤ git add .\n➤ git commit -am \"more models\"\n[dev a0fa9fa] more models\n 1 file changed, 4 insertions(+)\n\n➤ git push origin dev\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 329 bytes | 329.00 KiB/s, done.\nTotal 3 (delta 2), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nTo github.com:b-rodrigues/example_repo.git\n   8b2f04f..a0fa9fa  dev -> dev\n\nLet’s suppose we’re done with adding features to dev. Let’s checkout main:\n\n➤ git checkout main\n\nand now, let’s not pull from dev, but merge:\n\n➤ git merge dev\nAuto-merging my_script.R\nCONFLICT (content): Merge conflict in my_script.R\nAuto-merging new_script.R\nCONFLICT (add/add): Merge conflict in new_script.R\nAutomatic merge failed; fix conflicts and then commit the result.\n\nSome conflicts are in the file. Let’s take a look (because I’m in the terminal, I use cat to print the file to the terminal, but you can open it in RStudio):\n\n➤ cat my_script.R \nlibrary(ggplot2)\nlibrary(randomForest)\n\ndata(mtcars)\n\nggplot(data = mtcars) +\n  geom_point(aes(y = cyl, x = mpg))\n\nrf <- randomForest(hp ~ mpg, data = mtcars)\n<<<<<<< HEAD\n\nplot(rf)\n=======\n\nplot(rf)\n\nrf2 <- randomForest(hp ~ mpg + am + cyl, data = mtcars)\n\nplot(rf2)\n>>>>>>> dev\n\nLooks like I somehow added some newline somewhere and this caused the conflict. This is quite easy to solve, let’s make the script look like this:\n\nlibrary(ggplot2)\nlibrary(randomForest)\n\ndata(mtcars)\n\nggplot(data = mtcars) +\n  geom_point(aes(y = cyl, x = mpg))\n\nrf <- randomForest(hp ~ mpg, data = mtcars)\n\nplot(rf)\n\nrf2 <- randomForest(hp ~ mpg + am + cyl, data = mtcars)\n\nplot(rf2)\n\nWe can now simply commit and push. Merging can be simpler than pulling and rebasing, especially if you exclusively worked on dev and master has not seen any activity."
  },
  {
    "objectID": "04-git.html#contributing-to-someone-elses-repository",
    "href": "04-git.html#contributing-to-someone-elses-repository",
    "title": "3  Git",
    "section": "3.8 Contributing to someone else’s repository",
    "text": "3.8 Contributing to someone else’s repository\nIt is also possible to contribute to someone else’s repository; by this I mean someone who is not a colleague, and who did not invite you to his or her repository. So this means that you do not have writing rights to the repository and cannot push to it.\nThis is outside the scope of this course, but it is crucial that you understand this as well. For this reason, I highly recommend reading this link.\nOk, so this wraps up this chapter. Git is incredibly feature rich and complex, but as already discussed, it is NOT optional to know about Git in our trade. So now that you have some understanding of how it works, I suggest that you read the manual here. W3Schools has a great tutorial as well."
  }
]